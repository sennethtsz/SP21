---
title: "Homework-6"
output: pdf_document
---

# Question 9.1

**Using the same crime data set uscrime.txt as in Question 8.2, apply Principal Component Analysis and then create a regression model using the first few principal components. Specify your new model in terms of the original variables (not the principal components), and compare its quality to that of your solution to Question 8.2. You can use the R function prcomp for PCA. (Note that to first scale the data, you can include scale. = TRUE to scale as part of the PCA function. Don't forget that, to make a prediction for the new city, you'll need to unscale the coefficients (i.e., do the scaling calculation in reverse)!)**

```{r}
rm(list=ls())

library(GGally)
library(corrplot)
library(DAAG)

# load data
data = read.table("C:/Users/Admin/Desktop/MM/Homework 6/uscrime.txt", 
                  stringsAsFactors = FALSE,
                  header = TRUE)
head(data)


# visualizing correlated variables
corr_matrix = cor(data)
corrplot(corr_matrix, method='circle', is.corr = FALSE)
ggpairs(data, columns = c('Crime', 'Po1', 'Po2', 'Ed', 'NW', 'Wealth', 'Ineq'),
        lower=list(continuous='smooth'))


# pca
pca = prcomp(data[,1:15], scale.=TRUE)
summary(pca)
screeplot(pca, type='lines')


# building with 6 principal components
pca.data = cbind(pca$x[,1:6], data['Crime'])
model = lm(Crime~., data=pca.data)
summary(model)
SST = sum((data$Crime - mean(data$Crime))^2)
SSR = sum(model$residuals^2)
```

Model with 6 components yield R^2^= `r (1 - SSR/SST)` .

```{r}
cvmodel = cv.lm(pca.data, model, m=5)
SSR.cv = attr(cvmodel, 'ms')*nrow(pca.data)
```

5-fold cross validation of model yield R^2^= `r (1 - SSR.cv/SST)` .

```{r}
# building with 4 significant principal components
pca.data = cbind(pca$x[,1:6][,c(1, 2, 4, 5)], data['Crime'])
model = lm(Crime~., data=pca.data)
summary(model)
SSR = sum(model$residuals^2)
```

Model with 4 components yield R^2^= `r (1 - SSR/SST)` .

```{r}
cvmodel = cv.lm(pca.data, model, m=5)
SSR.cv = attr(cvmodel, 'ms')*nrow(pca.data)
```

5-fold cross validation of model yield R^2^= `r (1 - SSR.cv/SST)` .

Not using PCA seemed to yield better models that predicts and explains the variability of the data better.

```{r}
# reverse scaling
beta = model$coefficients
alpha.scaled = pca$rotation[,c(1, 2, 4, 5)] %*% beta[2:5]

alpha = alpha.scaled/sapply(data[,1:15], sd)
intercept = beta[1] - sum(alpha.scaled*sapply(data[,1:15], mean)/
                            sapply(data[,1:15], sd))

# Coefficients of unscaled variables
alpha

# Unscaled Intercept
intercept
```
