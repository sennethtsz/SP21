---
title: "Homework-2"
output: pdf_document
---

# Question 3.1a

**Using the same data set (credit_card_data.txt or credit_card_data-headers.txt) as in Question 2.2, use the ksvm or kknn function to find a good classifier:**

**(a) using cross-validation (do this for the k-nearest-neighbors model; SVM is optional); and**

**(b) splitting the data into training, validation, and test data sets (pick either KNN or SVM; the other is optional).**

**Import Packages**

```{r}
library(kernlab)
library(kknn)

set.seed(42069)
```

**Load Data**

```{r}
data <- read.table("C:/Users/Admin/Desktop/MM/Homework 2/credit_card_data.txt")

head(data)
```

**Cross validation data split 80:20**

```{r}
train.idx <- sample(nrow(data), size=nrow(data)*0.8)
train <- data[train.idx,]
test <- data[-train.idx,]
```

### **Leave-one-out cross validation method**

```{r}
kmax = 50 # number of k values across validation
kknn.loocv <- train.kknn(V11~.,
                         train,
                         kmax=kmax,
                         scale=TRUE)
acc.list <- list()
for (k in 1:kmax){
  predicted <- round(fitted(kknn.loocv)[[k]]) # returned predictions
  acc.list[k] <- sum(predicted == train[,11])/nrow(train)*100
}
```

```{r echo=FALSE}
print(paste('Highest accuracy achieved in validation:', max(unlist(acc.list)), '%'))
print(paste('Corresponding k-value validation:', which.max(acc.list)))
```

**Prediction on test set**

```{r}
kknn.loocv <- train.kknn(V11~.,
                         train,
                         ks=which.max(acc.list),
                         scale=TRUE)
predicted <- predict(kknn.loocv, test)
acc <- sum(round(predicted) == test[,11])/nrow(test)*100
```

```{r echo=FALSE}
print(paste('Accuracy on test set:', acc, '%'))
```

### **k-fold cross validation method**

```{r}
acc.list <- list()
for (k in 1:kmax){
  kknn.kfold <- cv.kknn(V11~.,
                        train,
                        k=k,
                        kcv=5, # 5-fold cross validation
                        scale=TRUE)
  predicted <- as.integer(round(kknn.kfold[[1]][,2]))
  acc <- sum(predicted == train[,11])/nrow(train)*100
  acc.list[k] <- acc
}
```

```{r echo=FALSE}
print(paste('Highest accuracy achieved in validation:', max(unlist(acc.list)), '%'))
print(paste('Corresponding k-value validation:', which.max(acc.list)))
```

**Prediction on test set**

```{r}
kknn.kfold <- kknn(V11~.,
                   train,
                   test,
                   k=which.max(acc.list),
                   scale=TRUE)
predicted = predict(kknn.kfold)
acc = sum(round(predicted) == test[,11])/nrow(test)*100
```

```{r echo=FALSE}
print(paste('Accuracy on test set:', acc, '%'))
```

\newpage

# Question 3.1b

**Import Packages**

```{r}
rm(list=ls())
library(kernlab)
library(kknn)

set.seed(42069)
```

**Load Data**

```{r}
data <- read.table("C:/Users/Admin/Desktop/MM/Homework 2/credit_card_data.txt")

head(data)
```

**Train, Valid, Test split 70:20:10**

```{r}
train.idx = sample(nrow(data), size=nrow(data)*0.7)
train = data[train.idx,] # train data set 70%

valid.test = data[-train.idx,] # valid & test 30%

valid.idx = sample(nrow(valid.test), size=nrow(valid.test)/3*2)
valid = valid.test[valid.idx,] # valid 20%
test = valid.test[-valid.idx,] # test 10%

train.valid.idx = c(valid.idx, train.idx)
train.valid = data[train.valid.idx,] # train & valid data for testing
```

### **Selecting kknn model on train and validation set**

```{r}
acc.list <- list()
for (k in 1:50){
  kknn.model <- kknn(V11~.,
                     train,
                     valid,
                     k=k,
                     scale=TRUE)
  predicted <- round(fitted(kknn.model))
  acc <- sum(predicted==valid[,11])/nrow(valid)*100
  acc.list[k] <- acc
}
```

```{r echo=FALSE}
print(paste('Highest accuracy achieved in validation:', max(unlist(acc.list)), '%'))
print(paste('Corresponding k-value validation:', which.max(acc.list)))
```

**Prediction on test data**

```{r}
kknn.model <- kknn(V11~.,
                   train,
                   test,
                   k=which.max(acc.list),
                   scale=TRUE)
predicted <- predict(kknn.model)
acc <- sum(round(predicted) == test[,11])/nrow(test)*100
```

```{r echo=FALSE}
print(paste('Accuracy on test set:', acc, '%'))
```

### **Selecting ksvm model on train and validation set**

```{r}
c <- c(0.001, 0.01, 0.1, 1, 10, 100, 1000)
acc.list <- list()
for (i in 1:length(c)){
  ksvm.model <- ksvm(x=as.matrix(train[,1:10]),
                     y=as.factor(train[,11]),
                     type='C-svc',
                     kernel='rbfdot',
                     C=c[i], # c selection in loop
                     scaled=TRUE)
  predicted <- predict(ksvm.model, valid[,1:10]) # train on valid set
  acc <- sum(predicted==valid[,11])/nrow(valid)*100
  acc.list[i] <- acc
}
```

```{r echo=FALSE}
print(paste('Highest accuracy achieved in validation:', max(unlist(acc.list)), '%'))
print(paste('Corresponding c-value with highest accuracy:', c[which.max(acc.list)]))
```

**Prediction on test data**

```{r}
ksvm.model <- ksvm(x=as.matrix(train.valid[,1:10]),
                   y=as.factor(train.valid[,11]),
                   type='C-svc',
                   kernel='rbfdot',
                   C=c[which.max(acc.list)],
                   scaled=TRUE)
predicted <- predict(ksvm.model, test[,1:10])
acc <- sum(predicted==test[,11])/nrow(test)*100
```

```{r echo=FALSE}
print(paste('Accuracy on test set:', acc, '%'))
```

\newpage

# Question 4.1

**Describe a situation or problem from your job, everyday life, current events, etc., for which a clustering model would be appropriate. List some (up to 5) predictors that you might use.**

In marketing analytics, it is easy to build a model to generalize customer behavior. However, not all customers behave the same. In this particular situation of credit card sales for example, model performance will increase significantly if different types of customers are identified first, before the application of any form of purchase propensity model. To cluster customers based on their behavior, we can look at:

1.  Customer credit score
2.  Spending patterns or habits in different categories of purchase using transactional data
3.  Net worth in their savings account
4.  Demographic information such as gender, number of family members, supplementary card spend, location, and etc.
5.  Customer responses to marketing advertisements, click through rate

\newpage

# Question 4.2

**The iris data set iris.txt contains 150 data points, each with four predictor variables and one categorical response. The predictors are the width and length of the sepal and petal of flowers and the response is the type of flower. The data is available from the R library datasets and can be accessed with iris once the library is loaded. It is also available at the UCI Machine Learning Repository (<https://archive.ics.uci.edu/ml/datasets/Iris> ). *The response values are only given to see how well a specific method performed and should not be used to build the model.***

**Use the R function kmeans to cluster the points as well as possible. Report the best combination of predictors, your suggested value of k, and how well your best clustering predicts**

**Import Packages**

```{r}
rm(list=ls())
library(kernlab)
library(ggplot2)

set.seed(42069)
```

**Load data**

```{r}
data = read.table("C:/Users/Admin/Desktop/MM/Homework 2/iris.txt")
head(data)
table(data[,5]) # tabulate species
```

**Visualize data**

```{r}
ggplot(data, aes(Petal.Length, Petal.Width, color = Species)) + geom_point()
ggplot(data, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point()
```

**Scaling data: (x-xmin)/(xmax-xmin)**

```{r}
data.scaled <- data
for (i in 1:4){
  data.scaled[i] <- (data[i]-min(data[i]))/(max(data[i]-min(data[i])))
}

head(data.scaled)
```

**Visualize scaled data**

```{r}
ggplot(data.scaled, aes(Petal.Length, Petal.Width, color = Species)) + geom_point()
ggplot(data.scaled, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point()
```

**Center selection**

```{r}
c <- c(1:10)
dist <- list()

for (i in 1:length(c)){
  model <- kmeans(as.matrix(data.scaled[,1:4]),
                  centers=i,
                  iter.max=10,
                  nstart=10)
  dist[i] <- model$tot.withinss
}
```

**Plot elbow chart to find optimal number of clusters**

```{r}
plot(c,
     dist,
     pch=19,
     frame=FALSE,
     type='b',
     xlab='Centers',
     ylab='Distance between points to centers in each cluster')
```

Overall distance between points to center begins to plateau at c=3.

**Model with 3 centers**

```{r}
model <- kmeans(as.matrix(data.scaled[,1:4]),
                centers=3,
                iter.max=10,
                nstart=10)

data.scaled[6] <- model$cluster
colnames(data.scaled)[6] <- 'Predicted'
```

**Visualize prediction**

```{r}
ggplot(data.scaled, aes(Petal.Length, Petal.Width, color = Predicted)) + geom_point()
ggplot(data.scaled, aes(Sepal.Length, Sepal.Width, color = Predicted)) + geom_point()
```
